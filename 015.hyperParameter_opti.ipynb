{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.util import shuffle_dataset\n",
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터와 시험 데이터를 load\n",
    "# 시험할것, 정답\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 빠르게 얻기 위해 훈련 데이터를 줄임\n",
    "x_train = x_train[:500]\n",
    "t_train = t_train[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20%를 검증 데이터로 분할\n",
    "validation_rate = 0.20\n",
    "validation_num = int(x_train.shape[0] * validation_rate)\n",
    "# 섞은후\n",
    "x_train, t_train = shuffle_dataset(x_train, t_train)\n",
    "# 자른다.\n",
    "x_val = x_train[:validation_num]\n",
    "t_val = t_train[:validation_num]\n",
    "x_train = x_train[validation_num:]\n",
    "t_train = t_train[validation_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameter : lr(학습률), weight_decay(가중치 감소), epocs(에폭은 기본 50으로)\n",
    "# epocs이란 것은 배치만큼 *한번* 학습망을 통화하며 트레이닝한 단위\n",
    "# train 함수\n",
    "def __train(lr, weight_decay, epocs=50):\n",
    "    network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                            output_size=10, weight_decay_lambda=weight_decay)\n",
    "    trainer = Trainer(network, x_train, t_train, x_val, t_val,\n",
    "                      epochs=epocs, mini_batch_size=100,\n",
    "                      optimizer='sgd', optimizer_param={'lr': lr}, verbose=False)\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer.test_acc_list, trainer.train_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_trial = 100  # 최적화 시도횟수\n",
    "results_val = {}          # 결과\n",
    "results_train = {}        # 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : val acc:0.08 | lr:0.00017702822593166194, weight decay:1.5252862931737688e-08\n",
      "1 : val acc:0.09 | lr:5.758057584381829e-05, weight decay:2.219185342958944e-07\n",
      "2 : val acc:0.08 | lr:9.987829488407817e-06, weight decay:8.593031440200376e-08\n",
      "3 : val acc:0.02 | lr:2.9067501641371875e-06, weight decay:8.946967236943471e-07\n",
      "4 : val acc:0.78 | lr:0.009343059442440339, weight decay:1.905471939321358e-08\n",
      "5 : val acc:0.12 | lr:1.588262470017465e-05, weight decay:2.3499546341145764e-07\n",
      "6 : val acc:0.16 | lr:2.6089567764362987e-05, weight decay:2.3286360958150222e-07\n",
      "7 : val acc:0.09 | lr:0.00020693756085455038, weight decay:5.110425011102715e-06\n",
      "8 : val acc:0.11 | lr:0.00036628442187792244, weight decay:2.2051677861145347e-08\n",
      "9 : val acc:0.08 | lr:0.00017027558688626293, weight decay:5.975787008872115e-08\n",
      "10 : val acc:0.05 | lr:5.111794225558727e-06, weight decay:1.0660834814714081e-05\n",
      "11 : val acc:0.09 | lr:2.9832633685187496e-06, weight decay:1.711691659984403e-08\n",
      "12 : val acc:0.24 | lr:0.0019904593014308586, weight decay:2.809932988262791e-05\n",
      "13 : val acc:0.18 | lr:0.001311923522081299, weight decay:3.6958082474802404e-07\n",
      "14 : val acc:0.29 | lr:0.00032140608124333654, weight decay:1.8304105713757835e-07\n",
      "15 : val acc:0.14 | lr:0.00032449327364364233, weight decay:3.7911936392144314e-07\n",
      "16 : val acc:0.11 | lr:1.467117473995843e-05, weight decay:1.5569505069116663e-06\n",
      "17 : val acc:0.18 | lr:0.0007911271150677027, weight decay:6.505481629377662e-05\n",
      "18 : val acc:0.09 | lr:1.8311989307439807e-05, weight decay:3.738847728405496e-07\n",
      "19 : val acc:0.05 | lr:5.616062516404932e-05, weight decay:1.0260332162015635e-07\n",
      "20 : val acc:0.12 | lr:3.113697641829585e-05, weight decay:7.829723800796063e-08\n",
      "21 : val acc:0.16 | lr:1.4556515282831693e-06, weight decay:6.501227282274761e-05\n",
      "22 : val acc:0.21 | lr:0.0008482082291977635, weight decay:2.259839151433876e-07\n",
      "23 : val acc:0.18 | lr:0.0009447008146750798, weight decay:4.649914272470221e-05\n",
      "24 : val acc:0.1 | lr:0.00045396682174623284, weight decay:2.0739128557573024e-06\n",
      "25 : val acc:0.29 | lr:0.002219683960912474, weight decay:7.035457358145415e-05\n",
      "26 : val acc:0.14 | lr:1.287604813295417e-05, weight decay:1.0298459956942656e-06\n",
      "27 : val acc:0.36 | lr:0.002310866827198348, weight decay:1.638066158833208e-06\n",
      "28 : val acc:0.13 | lr:3.438529275940343e-05, weight decay:2.6549504901094485e-08\n",
      "29 : val acc:0.46 | lr:0.0020716740173278166, weight decay:1.5845991702828336e-06\n",
      "30 : val acc:0.14 | lr:0.00012104551567674638, weight decay:3.2139636769263314e-07\n",
      "31 : val acc:0.16 | lr:9.895055150260622e-06, weight decay:4.003955204150927e-08\n",
      "32 : val acc:0.06 | lr:4.230380162710751e-06, weight decay:1.4556437251549845e-05\n",
      "33 : val acc:0.13 | lr:0.00011472849349831875, weight decay:2.1324197884485514e-07\n",
      "34 : val acc:0.11 | lr:0.0004603270676299559, weight decay:3.385903745730942e-08\n",
      "35 : val acc:0.8 | lr:0.009140655345812295, weight decay:2.826960421006667e-08\n",
      "36 : val acc:0.12 | lr:8.518270210239006e-06, weight decay:2.250903113650568e-06\n",
      "37 : val acc:0.15 | lr:1.305570394758153e-06, weight decay:2.0869573555969025e-05\n",
      "38 : val acc:0.16 | lr:0.0001800202237459379, weight decay:2.4340523036062354e-08\n",
      "39 : val acc:0.17 | lr:0.0007179764643478494, weight decay:2.2479833214521936e-08\n",
      "40 : val acc:0.17 | lr:0.0003003230124817811, weight decay:4.139394347299242e-08\n",
      "41 : val acc:0.06 | lr:2.8620453395729835e-05, weight decay:3.757085854429691e-08\n",
      "42 : val acc:0.18 | lr:4.193434135687785e-05, weight decay:1.3996913498909816e-08\n",
      "43 : val acc:0.14 | lr:9.728853868841268e-05, weight decay:5.120448335312935e-07\n",
      "44 : val acc:0.1 | lr:1.9529780053530797e-06, weight decay:1.55925696797841e-08\n",
      "45 : val acc:0.23 | lr:0.0011039958517239175, weight decay:2.7539059411703772e-05\n",
      "46 : val acc:0.12 | lr:0.00028323290983391504, weight decay:1.3522207914839862e-06\n",
      "47 : val acc:0.02 | lr:1.24337163378353e-06, weight decay:4.117974477197232e-07\n",
      "48 : val acc:0.11 | lr:3.655157518699708e-05, weight decay:1.2037056250961385e-05\n",
      "49 : val acc:0.1 | lr:5.635163439002145e-05, weight decay:5.627225331274259e-08\n",
      "50 : val acc:0.15 | lr:4.43304556754096e-05, weight decay:3.1633071483342967e-06\n",
      "51 : val acc:0.08 | lr:3.7737710636381524e-06, weight decay:8.05031757981185e-05\n",
      "52 : val acc:0.13 | lr:8.250816078057853e-06, weight decay:2.7076723557422968e-05\n",
      "53 : val acc:0.09 | lr:1.4333378255800187e-06, weight decay:9.873623784373774e-08\n",
      "54 : val acc:0.12 | lr:0.00013353241307169185, weight decay:9.835531243140046e-07\n",
      "55 : val acc:0.23 | lr:0.0011983528837652233, weight decay:2.8994188835557557e-07\n",
      "56 : val acc:0.07 | lr:1.4990651314454582e-06, weight decay:2.279599179093438e-06\n",
      "57 : val acc:0.08 | lr:0.0007335061307506879, weight decay:1.4716633481250513e-07\n",
      "58 : val acc:0.71 | lr:0.006204837886935036, weight decay:1.2803854451019182e-08\n",
      "59 : val acc:0.03 | lr:3.97250971795333e-06, weight decay:5.284521010711538e-08\n",
      "60 : val acc:0.12 | lr:0.000946650151176462, weight decay:8.175629704947e-08\n",
      "61 : val acc:0.11 | lr:1.8389899491643085e-05, weight decay:4.578372463150778e-05\n",
      "62 : val acc:0.14 | lr:0.001331374709651405, weight decay:4.4995961639141744e-07\n",
      "63 : val acc:0.18 | lr:0.0009605426879149584, weight decay:4.606248256647831e-06\n",
      "64 : val acc:0.04 | lr:9.712426915523738e-05, weight decay:9.1812869005242e-06\n",
      "65 : val acc:0.1 | lr:0.0005396365924534228, weight decay:2.8246907743522518e-08\n",
      "66 : val acc:0.11 | lr:0.0002575534802180695, weight decay:2.6717282020116954e-05\n",
      "67 : val acc:0.12 | lr:1.9841740528234265e-05, weight decay:4.866092499400852e-06\n",
      "68 : val acc:0.37 | lr:0.0017780774011395173, weight decay:3.93714764706696e-06\n",
      "69 : val acc:0.17 | lr:0.0006891196633558372, weight decay:1.508494810178066e-08\n",
      "70 : val acc:0.09 | lr:0.00017374133049485998, weight decay:3.713392879752516e-07\n",
      "71 : val acc:0.5 | lr:0.002599258496032049, weight decay:3.428527970704319e-08\n",
      "72 : val acc:0.1 | lr:0.0002825832379166336, weight decay:4.065811626278115e-07\n",
      "73 : val acc:0.37 | lr:0.0014421928941592875, weight decay:9.186111349289674e-06\n",
      "74 : val acc:0.06 | lr:0.00023652027835179603, weight decay:8.307704154852672e-05\n",
      "75 : val acc:0.09 | lr:1.204737882087207e-05, weight decay:3.061722049847594e-05\n",
      "76 : val acc:0.33 | lr:0.0008638474630048887, weight decay:7.258157795686478e-05\n",
      "77 : val acc:0.06 | lr:3.7582026882735926e-05, weight decay:2.583568826119525e-06\n",
      "78 : val acc:0.82 | lr:0.009523707707182699, weight decay:2.529441821565897e-07\n",
      "79 : val acc:0.11 | lr:3.011763722830757e-06, weight decay:4.0914364002163893e-07\n",
      "80 : val acc:0.14 | lr:2.4817181494013896e-05, weight decay:5.543743126736646e-08\n",
      "81 : val acc:0.11 | lr:4.106273393064773e-06, weight decay:5.182352647227734e-07\n",
      "82 : val acc:0.72 | lr:0.0052577953456525725, weight decay:4.681458595123731e-07\n",
      "83 : val acc:0.13 | lr:4.4567974225704356e-05, weight decay:1.3121672794373287e-08\n",
      "84 : val acc:0.17 | lr:8.255476516645476e-06, weight decay:2.372997868038406e-07\n",
      "85 : val acc:0.09 | lr:3.8212972175910246e-06, weight decay:3.878693980592577e-06\n",
      "86 : val acc:0.72 | lr:0.005956040665099473, weight decay:6.574492890615886e-05\n",
      "87 : val acc:0.06 | lr:8.798444598322846e-06, weight decay:3.037809027553134e-08\n",
      "88 : val acc:0.2 | lr:1.0965946964185449e-06, weight decay:1.6422463548597293e-06\n",
      "89 : val acc:0.17 | lr:0.0005540019838727864, weight decay:3.846396539049686e-08\n",
      "90 : val acc:0.01 | lr:4.1953417446154415e-06, weight decay:2.8781670227542967e-06\n",
      "91 : val acc:0.45 | lr:0.0018218550229269436, weight decay:7.09725034476882e-06\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터 무작위 탐색======================================\n",
    "for i in range(optimization_trial):\n",
    "    # 탐색할 하이퍼파라미터의 범위 지정===============\n",
    "    weight_decay = 10 ** np.random.uniform(-8, -4)\n",
    "    lr = 10 ** np.random.uniform(-6, -2)\n",
    "    # ================================================\n",
    "    \n",
    "    #val은 검증데이터의 accuracy\n",
    "    # lr과 decay값을 키로,, value는 에폭당 정확도, 에폭은 50씩만 돌려본다.\n",
    "    val_acc_list, train_acc_list = __train(lr, weight_decay)\n",
    "    print(i,\": val acc:\" + str(val_acc_list[-1]) + \" | lr:\" + str(lr) + \", weight decay:\" + str(weight_decay))\n",
    "    key = \"lr:\" + str(lr) + \", weight decay:\" + str(weight_decay)\n",
    "    results_val[key] = val_acc_list\n",
    "    results_train[key] = train_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 그리기========================================================\n",
    "print(\"=========== Hyper-Parameter Optimization Result ===========\")\n",
    "graph_draw_num = 20\n",
    "col_num = 5\n",
    "row_num = int(np.ceil(graph_draw_num / col_num))\n",
    "i = 0\n",
    "\n",
    "# key = 가장 마지막 학습된 정확도를 기준으로 reverse sort\n",
    "for key, val_acc_list in sorted(results_val.items(), key=lambda x:x[1][-1], reverse=True):\n",
    "    print(\"Best-\" + str(i+1) + \"(val acc:\" + str(val_acc_list[-1]) + \") | \" + key)\n",
    "\n",
    "    plt.subplot(row_num, col_num, i+1)\n",
    "    plt.title(\"Best-\" + str(i+1))\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    if i % 5: plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    x = np.arange(len(val_acc_list))\n",
    "    plt.plot(x, val_acc_list)\n",
    "    plt.plot(x, results_train[key], \"--\")\n",
    "    i += 1\n",
    "\n",
    "    if i >= graph_draw_num:\n",
    "        break\n",
    "        \n",
    "# 각 best마다, 에폭증가당 정확도 증가량 그래프\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
