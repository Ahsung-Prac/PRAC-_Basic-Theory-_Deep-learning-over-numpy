{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2930650051877426\n",
      "=== epoch:1, train acc:0.1, test acc:0.1027 ===\n",
      "train loss:2.2991656570560086\n",
      "train loss:2.296289763579566\n",
      "train loss:2.2995056916641587\n",
      "=== epoch:2, train acc:0.11333333333333333, test acc:0.1001 ===\n",
      "train loss:2.2992793980921085\n",
      "train loss:2.300296175573571\n",
      "train loss:2.2896389544839955\n",
      "=== epoch:3, train acc:0.11, test acc:0.102 ===\n",
      "train loss:2.296030351002336\n",
      "train loss:2.2908357066638843\n",
      "train loss:2.294394952686566\n",
      "=== epoch:4, train acc:0.11333333333333333, test acc:0.1021 ===\n",
      "train loss:2.3065029258227403\n",
      "train loss:2.2899644282725573\n",
      "train loss:2.2949512628341253\n",
      "=== epoch:5, train acc:0.12, test acc:0.1021 ===\n",
      "train loss:2.286085819929808\n",
      "train loss:2.2966774016642635\n",
      "train loss:2.2945738513319087\n",
      "=== epoch:6, train acc:0.12666666666666668, test acc:0.1031 ===\n",
      "train loss:2.2929115428407436\n",
      "train loss:2.2923115122815876\n",
      "train loss:2.3025125414512995\n",
      "=== epoch:7, train acc:0.13666666666666666, test acc:0.1034 ===\n",
      "train loss:2.2932376586051135\n",
      "train loss:2.285966022984998\n",
      "train loss:2.28914725302585\n",
      "=== epoch:8, train acc:0.14666666666666667, test acc:0.1052 ===\n",
      "train loss:2.2941223710797107\n",
      "train loss:2.2824094959645165\n",
      "train loss:2.289920520422388\n",
      "=== epoch:9, train acc:0.14, test acc:0.1065 ===\n",
      "train loss:2.2858484061268918\n",
      "train loss:2.2978435812168563\n",
      "train loss:2.2892815161053917\n",
      "=== epoch:10, train acc:0.13, test acc:0.1096 ===\n",
      "train loss:2.2756528067227038\n",
      "train loss:2.27665353097309\n",
      "train loss:2.2852192742195574\n",
      "=== epoch:11, train acc:0.13333333333333333, test acc:0.1113 ===\n",
      "train loss:2.2803398047132153\n",
      "train loss:2.278906311212835\n",
      "train loss:2.274121197733609\n",
      "=== epoch:12, train acc:0.13333333333333333, test acc:0.1135 ===\n",
      "train loss:2.2745684174502103\n",
      "train loss:2.2806622469578715\n",
      "train loss:2.2889392244215223\n",
      "=== epoch:13, train acc:0.14333333333333334, test acc:0.1166 ===\n",
      "train loss:2.2779557308058695\n",
      "train loss:2.2764490485343187\n",
      "train loss:2.289009482127771\n",
      "=== epoch:14, train acc:0.15333333333333332, test acc:0.1185 ===\n",
      "train loss:2.2778818764354183\n",
      "train loss:2.2785455074499854\n",
      "train loss:2.2742172557008833\n",
      "=== epoch:15, train acc:0.16, test acc:0.122 ===\n",
      "train loss:2.2856316315526026\n",
      "train loss:2.2897804448279784\n",
      "train loss:2.280323683008345\n",
      "=== epoch:16, train acc:0.16333333333333333, test acc:0.1255 ===\n",
      "train loss:2.280163408670644\n",
      "train loss:2.2773108609040644\n",
      "train loss:2.2820629265199264\n",
      "=== epoch:17, train acc:0.16333333333333333, test acc:0.1296 ===\n",
      "train loss:2.275905745220934\n",
      "train loss:2.2793746804587363\n",
      "train loss:2.290713993061106\n",
      "=== epoch:18, train acc:0.16, test acc:0.1356 ===\n",
      "train loss:2.276126236762402\n",
      "train loss:2.275381542627892\n",
      "train loss:2.2778231199147463\n",
      "=== epoch:19, train acc:0.17333333333333334, test acc:0.1409 ===\n",
      "train loss:2.2707065094756196\n",
      "train loss:2.291316222137585\n",
      "train loss:2.2747247078502553\n",
      "=== epoch:20, train acc:0.17666666666666667, test acc:0.1429 ===\n",
      "train loss:2.2842503265374416\n",
      "train loss:2.2754197445942945\n",
      "train loss:2.2819881791834717\n",
      "=== epoch:21, train acc:0.18, test acc:0.1521 ===\n",
      "train loss:2.2619371675419093\n",
      "train loss:2.2677655495506297\n",
      "train loss:2.2865850710247226\n",
      "=== epoch:22, train acc:0.19666666666666666, test acc:0.1605 ===\n",
      "train loss:2.282456504776543\n",
      "train loss:2.2706231888255597\n",
      "train loss:2.2595334700333223\n",
      "=== epoch:23, train acc:0.20333333333333334, test acc:0.1671 ===\n",
      "train loss:2.2710035561092186\n",
      "train loss:2.2758134582864313\n",
      "train loss:2.2779496211969232\n",
      "=== epoch:24, train acc:0.19666666666666666, test acc:0.1703 ===\n",
      "train loss:2.2669072813689324\n",
      "train loss:2.2681980012517\n",
      "train loss:2.2713190701021384\n",
      "=== epoch:25, train acc:0.21, test acc:0.1801 ===\n",
      "train loss:2.2713243212464427\n",
      "train loss:2.275512044124265\n",
      "train loss:2.274051419433354\n",
      "=== epoch:26, train acc:0.21666666666666667, test acc:0.1869 ===\n",
      "train loss:2.283129267148261\n",
      "train loss:2.2871205190203283\n",
      "train loss:2.259929546553806\n",
      "=== epoch:27, train acc:0.22333333333333333, test acc:0.1887 ===\n",
      "train loss:2.2718816200930387\n",
      "train loss:2.270373202233077\n",
      "train loss:2.2633059663591073\n",
      "=== epoch:28, train acc:0.22666666666666666, test acc:0.1926 ===\n",
      "train loss:2.282322970221969\n",
      "train loss:2.2767294272042444\n",
      "train loss:2.260976106866636\n",
      "=== epoch:29, train acc:0.22, test acc:0.1969 ===\n",
      "train loss:2.25435610583782\n",
      "train loss:2.266288185626179\n",
      "train loss:2.2761544379712557\n",
      "=== epoch:30, train acc:0.23333333333333334, test acc:0.201 ===\n",
      "train loss:2.2680800258826737\n",
      "train loss:2.2527162616602756\n",
      "train loss:2.273687650077355\n",
      "=== epoch:31, train acc:0.24, test acc:0.2049 ===\n",
      "train loss:2.2679825102593933\n",
      "train loss:2.2593677238550725\n",
      "train loss:2.2684699306738803\n",
      "=== epoch:32, train acc:0.23333333333333334, test acc:0.2047 ===\n",
      "train loss:2.2626283202232145\n",
      "train loss:2.269917762453177\n",
      "train loss:2.279668902419578\n",
      "=== epoch:33, train acc:0.24333333333333335, test acc:0.2081 ===\n",
      "train loss:2.258199326373755\n",
      "train loss:2.2388018308161235\n",
      "train loss:2.2684759165856816\n",
      "=== epoch:34, train acc:0.24333333333333335, test acc:0.2088 ===\n",
      "train loss:2.269859997753795\n",
      "train loss:2.2665631064952554\n",
      "train loss:2.244216928424441\n",
      "=== epoch:35, train acc:0.24333333333333335, test acc:0.2086 ===\n",
      "train loss:2.2652430386861147\n",
      "train loss:2.2666294675439627\n",
      "train loss:2.2660397148889624\n",
      "=== epoch:36, train acc:0.25666666666666665, test acc:0.2102 ===\n",
      "train loss:2.238076492481348\n",
      "train loss:2.266165280599548\n",
      "train loss:2.266487406298311\n",
      "=== epoch:37, train acc:0.26, test acc:0.2091 ===\n",
      "train loss:2.2704582610985526\n",
      "train loss:2.271349581504951\n",
      "train loss:2.249083791519672\n",
      "=== epoch:38, train acc:0.26, test acc:0.21 ===\n",
      "train loss:2.2612133445800024\n",
      "train loss:2.2645548235814195\n",
      "train loss:2.263883127046087\n",
      "=== epoch:39, train acc:0.26666666666666666, test acc:0.2118 ===\n",
      "train loss:2.260841850838928\n",
      "train loss:2.25672103121956\n",
      "train loss:2.2616753662794253\n",
      "=== epoch:40, train acc:0.25333333333333335, test acc:0.2132 ===\n",
      "train loss:2.2506485185443146\n",
      "train loss:2.2538833564407166\n",
      "train loss:2.256054425860725\n",
      "=== epoch:41, train acc:0.25666666666666665, test acc:0.2117 ===\n",
      "train loss:2.2721517480956486\n",
      "train loss:2.251779084514696\n",
      "train loss:2.233567131756685\n",
      "=== epoch:42, train acc:0.25, test acc:0.211 ===\n",
      "train loss:2.256393405881511\n",
      "train loss:2.251936467344706\n",
      "train loss:2.26923537776053\n",
      "=== epoch:43, train acc:0.25, test acc:0.2112 ===\n",
      "train loss:2.244770788529394\n",
      "train loss:2.263730426644739\n",
      "train loss:2.2583795981483994\n",
      "=== epoch:44, train acc:0.26666666666666666, test acc:0.2129 ===\n",
      "train loss:2.255310605240837\n",
      "train loss:2.2625782434078148\n",
      "train loss:2.239872842032512\n",
      "=== epoch:45, train acc:0.26666666666666666, test acc:0.2137 ===\n",
      "train loss:2.252787783549808\n",
      "train loss:2.2532532289610483\n",
      "train loss:2.2603633415348248\n",
      "=== epoch:46, train acc:0.26, test acc:0.2136 ===\n",
      "train loss:2.2464476774799946\n",
      "train loss:2.250337750642927\n",
      "train loss:2.2503553569143055\n",
      "=== epoch:47, train acc:0.27, test acc:0.2149 ===\n",
      "train loss:2.2533312703183173\n",
      "train loss:2.2641562420176298\n",
      "train loss:2.258867226605234\n",
      "=== epoch:48, train acc:0.27666666666666667, test acc:0.2157 ===\n",
      "train loss:2.245933134075676\n",
      "train loss:2.2547937204624025\n",
      "train loss:2.2742437166419807\n",
      "=== epoch:49, train acc:0.2733333333333333, test acc:0.2153 ===\n",
      "train loss:2.2445760734049123\n",
      "train loss:2.2565006521136697\n",
      "train loss:2.2504883095970305\n",
      "=== epoch:50, train acc:0.2833333333333333, test acc:0.2194 ===\n",
      "train loss:2.2606076824773087\n",
      "train loss:2.248565677194905\n",
      "train loss:2.2597123686157623\n",
      "=== epoch:51, train acc:0.29, test acc:0.2206 ===\n",
      "train loss:2.2476824662223684\n",
      "train loss:2.247653924562596\n",
      "train loss:2.259813612206135\n",
      "=== epoch:52, train acc:0.2866666666666667, test acc:0.2236 ===\n",
      "train loss:2.259354375343178\n",
      "train loss:2.252823870255428\n",
      "train loss:2.254895332106271\n",
      "=== epoch:53, train acc:0.2966666666666667, test acc:0.2264 ===\n",
      "train loss:2.2588096996259095\n",
      "train loss:2.2393975791133656\n",
      "train loss:2.2570557627921746\n",
      "=== epoch:54, train acc:0.30333333333333334, test acc:0.2276 ===\n",
      "train loss:2.225727965849676\n",
      "train loss:2.251438326089092\n",
      "train loss:2.2640537979912\n",
      "=== epoch:55, train acc:0.30666666666666664, test acc:0.2308 ===\n",
      "train loss:2.234513925701299\n",
      "train loss:2.2470845173335734\n",
      "train loss:2.2488174749780137\n",
      "=== epoch:56, train acc:0.30666666666666664, test acc:0.2304 ===\n",
      "train loss:2.2454143297230056\n",
      "train loss:2.2456215870322027\n",
      "train loss:2.2352697069761795\n",
      "=== epoch:57, train acc:0.31, test acc:0.232 ===\n",
      "train loss:2.2605502565315763\n",
      "train loss:2.238448939351164\n",
      "train loss:2.2426468335965146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:58, train acc:0.31333333333333335, test acc:0.2337 ===\n",
      "train loss:2.231586003837442\n",
      "train loss:2.2620592792055834\n",
      "train loss:2.247190939154441\n",
      "=== epoch:59, train acc:0.31666666666666665, test acc:0.2353 ===\n",
      "train loss:2.2325086537293535\n",
      "train loss:2.2462428904605063\n",
      "train loss:2.2229966731280966\n",
      "=== epoch:60, train acc:0.32, test acc:0.2356 ===\n",
      "train loss:2.232783226795808\n",
      "train loss:2.239722330835308\n",
      "train loss:2.249212884931649\n",
      "=== epoch:61, train acc:0.3233333333333333, test acc:0.2362 ===\n",
      "train loss:2.2481813455577084\n",
      "train loss:2.245600378970257\n",
      "train loss:2.23546419075239\n",
      "=== epoch:62, train acc:0.3233333333333333, test acc:0.2374 ===\n",
      "train loss:2.223903861902236\n",
      "train loss:2.259911274025204\n",
      "train loss:2.236764922637585\n",
      "=== epoch:63, train acc:0.32666666666666666, test acc:0.2371 ===\n",
      "train loss:2.2202940134046907\n",
      "train loss:2.2364327171626615\n",
      "train loss:2.241605502414463\n",
      "=== epoch:64, train acc:0.32666666666666666, test acc:0.2379 ===\n",
      "train loss:2.2455285245073875\n",
      "train loss:2.237038700099131\n",
      "train loss:2.2285828351692967\n",
      "=== epoch:65, train acc:0.3233333333333333, test acc:0.2352 ===\n",
      "train loss:2.2068985528435228\n",
      "train loss:2.2313500261194847\n",
      "train loss:2.232548692333266\n",
      "=== epoch:66, train acc:0.32, test acc:0.2347 ===\n",
      "train loss:2.2151640173328273\n",
      "train loss:2.218013357677652\n",
      "train loss:2.243455860771836\n",
      "=== epoch:67, train acc:0.3233333333333333, test acc:0.2343 ===\n",
      "train loss:2.21782644973361\n",
      "train loss:2.2256216343749933\n",
      "train loss:2.2284049089216076\n",
      "=== epoch:68, train acc:0.32, test acc:0.234 ===\n",
      "train loss:2.238222162976448\n",
      "train loss:2.2129105247995007\n",
      "train loss:2.221054859140393\n",
      "=== epoch:69, train acc:0.32, test acc:0.2333 ===\n",
      "train loss:2.2089781410054163\n",
      "train loss:2.230142506849309\n",
      "train loss:2.232684702571968\n",
      "=== epoch:70, train acc:0.31666666666666665, test acc:0.2342 ===\n",
      "train loss:2.227948604175498\n",
      "train loss:2.2172466368225083\n",
      "train loss:2.2444853828424365\n",
      "=== epoch:71, train acc:0.31666666666666665, test acc:0.236 ===\n",
      "train loss:2.2379235109698246\n",
      "train loss:2.200114162585284\n",
      "train loss:2.2357440160136632\n",
      "=== epoch:72, train acc:0.31666666666666665, test acc:0.2345 ===\n",
      "train loss:2.2474349477516253\n",
      "train loss:2.226253945280476\n",
      "train loss:2.22361134279152\n",
      "=== epoch:73, train acc:0.32, test acc:0.2354 ===\n",
      "train loss:2.2409573566061214\n",
      "train loss:2.2415977026440803\n",
      "train loss:2.229175160086834\n",
      "=== epoch:74, train acc:0.31666666666666665, test acc:0.2376 ===\n",
      "train loss:2.1900134033518426\n",
      "train loss:2.2332637478043815\n",
      "train loss:2.2182754004290772\n",
      "=== epoch:75, train acc:0.31333333333333335, test acc:0.2347 ===\n",
      "train loss:2.226213319665887\n",
      "train loss:2.211085745152312\n",
      "train loss:2.2024891383640677\n",
      "=== epoch:76, train acc:0.31, test acc:0.2339 ===\n",
      "train loss:2.2436148676497973\n",
      "train loss:2.2363359278068704\n",
      "train loss:2.208506523567677\n",
      "=== epoch:77, train acc:0.31333333333333335, test acc:0.2355 ===\n",
      "train loss:2.216324087889863\n",
      "train loss:2.219785155749916\n",
      "train loss:2.1772571936567373\n",
      "=== epoch:78, train acc:0.31333333333333335, test acc:0.2342 ===\n",
      "train loss:2.228970262608774\n",
      "train loss:2.211580106529055\n",
      "train loss:2.222767380338959\n",
      "=== epoch:79, train acc:0.31666666666666665, test acc:0.2366 ===\n",
      "train loss:2.219788039540845\n",
      "train loss:2.218055504754023\n",
      "train loss:2.199791999758346\n",
      "=== epoch:80, train acc:0.32, test acc:0.2394 ===\n",
      "train loss:2.2080358992086926\n",
      "train loss:2.256080062007146\n",
      "train loss:2.2001785644125182\n",
      "=== epoch:81, train acc:0.3233333333333333, test acc:0.2421 ===\n",
      "train loss:2.2089286399710373\n",
      "train loss:2.2034271884995316\n",
      "train loss:2.211382316074376\n",
      "=== epoch:82, train acc:0.3233333333333333, test acc:0.2428 ===\n",
      "train loss:2.191775457244466\n",
      "train loss:2.2118920829079753\n",
      "train loss:2.215319209789148\n",
      "=== epoch:83, train acc:0.3233333333333333, test acc:0.2417 ===\n",
      "train loss:2.209818587296071\n",
      "train loss:2.2040511640822644\n",
      "train loss:2.2321230592613275\n",
      "=== epoch:84, train acc:0.3233333333333333, test acc:0.2433 ===\n",
      "train loss:2.2001209236616703\n",
      "train loss:2.2233069450777707\n",
      "train loss:2.206194742376999\n",
      "=== epoch:85, train acc:0.3233333333333333, test acc:0.2446 ===\n",
      "train loss:2.184337920710385\n",
      "train loss:2.2075352411790363\n",
      "train loss:2.200213773340225\n",
      "=== epoch:86, train acc:0.32666666666666666, test acc:0.2428 ===\n",
      "train loss:2.2239760905337143\n",
      "train loss:2.204281285786399\n",
      "train loss:2.205709799616486\n",
      "=== epoch:87, train acc:0.32666666666666666, test acc:0.2406 ===\n",
      "train loss:2.1915079983106533\n",
      "train loss:2.1949372854478675\n",
      "train loss:2.2433611425300244\n",
      "=== epoch:88, train acc:0.33, test acc:0.2413 ===\n",
      "train loss:2.2113675838693334\n",
      "train loss:2.209556064696479\n",
      "train loss:2.2094126600015414\n",
      "=== epoch:89, train acc:0.32666666666666666, test acc:0.2439 ===\n",
      "train loss:2.2012928264472653\n",
      "train loss:2.212676998731404\n",
      "train loss:2.1738048346715333\n",
      "=== epoch:90, train acc:0.33, test acc:0.2458 ===\n",
      "train loss:2.2287341604213338\n",
      "train loss:2.195439485809036\n",
      "train loss:2.1932841295828207\n",
      "=== epoch:91, train acc:0.3333333333333333, test acc:0.2456 ===\n",
      "train loss:2.200310928702943\n",
      "train loss:2.215165544033578\n",
      "train loss:2.2137334838126166\n",
      "=== epoch:92, train acc:0.3333333333333333, test acc:0.2469 ===\n",
      "train loss:2.187607033206371\n",
      "train loss:2.197986519347182\n",
      "train loss:2.2163435140403056\n",
      "=== epoch:93, train acc:0.33, test acc:0.247 ===\n",
      "train loss:2.213323836429673\n",
      "train loss:2.201335673936815\n",
      "train loss:2.1628458585991948\n",
      "=== epoch:94, train acc:0.33, test acc:0.2466 ===\n",
      "train loss:2.230900749390341\n",
      "train loss:2.177891496305609\n",
      "train loss:2.189726036217504\n",
      "=== epoch:95, train acc:0.3333333333333333, test acc:0.2481 ===\n",
      "train loss:2.1915915397500503\n",
      "train loss:2.1857056839047884\n",
      "train loss:2.1823318667083633\n",
      "=== epoch:96, train acc:0.33, test acc:0.247 ===\n",
      "train loss:2.1812157237395633\n",
      "train loss:2.2247178033629837\n",
      "train loss:2.178368855944021\n",
      "=== epoch:97, train acc:0.33, test acc:0.25 ===\n",
      "train loss:2.172362776762538\n",
      "train loss:2.1740075317868808\n",
      "train loss:2.195710521267802\n",
      "=== epoch:98, train acc:0.3333333333333333, test acc:0.2521 ===\n",
      "train loss:2.186223337193395\n",
      "train loss:2.182397302878623\n",
      "train loss:2.1870580539701923\n",
      "=== epoch:99, train acc:0.3333333333333333, test acc:0.2527 ===\n",
      "train loss:2.1781150208932045\n",
      "train loss:2.173749672839912\n",
      "train loss:2.17012622979231\n",
      "=== epoch:100, train acc:0.32666666666666666, test acc:0.2504 ===\n",
      "train loss:2.1785854246774186\n",
      "train loss:2.1807894261916463\n",
      "train loss:2.1928536068012\n",
      "=== epoch:101, train acc:0.32666666666666666, test acc:0.2535 ===\n",
      "train loss:2.188276867496728\n",
      "train loss:2.1629089199734004\n",
      "train loss:2.196700597819995\n",
      "=== epoch:102, train acc:0.33666666666666667, test acc:0.2526 ===\n",
      "train loss:2.164672021981469\n",
      "train loss:2.152184620870806\n",
      "train loss:2.1686833157988357\n",
      "=== epoch:103, train acc:0.32666666666666666, test acc:0.2498 ===\n",
      "train loss:2.159855360313783\n",
      "train loss:2.1560062932946273\n",
      "train loss:2.1631097185739816\n",
      "=== epoch:104, train acc:0.32, test acc:0.2469 ===\n",
      "train loss:2.177007143993853\n",
      "train loss:2.2027969155319815\n",
      "train loss:2.1545591239947743\n",
      "=== epoch:105, train acc:0.30666666666666664, test acc:0.2449 ===\n",
      "train loss:2.134305675480308\n",
      "train loss:2.165704229320145\n",
      "train loss:2.1913538591216253\n",
      "=== epoch:106, train acc:0.31, test acc:0.246 ===\n",
      "train loss:2.1468611534418005\n",
      "train loss:2.177603345883689\n",
      "train loss:2.189883610198639\n",
      "=== epoch:107, train acc:0.3233333333333333, test acc:0.2478 ===\n",
      "train loss:2.1701436758785713\n",
      "train loss:2.1589864041420945\n",
      "train loss:2.162358350995141\n",
      "=== epoch:108, train acc:0.31666666666666665, test acc:0.2476 ===\n",
      "train loss:2.176067130158751\n",
      "train loss:2.1679420367858917\n",
      "train loss:2.148382994559134\n",
      "=== epoch:109, train acc:0.31, test acc:0.2482 ===\n",
      "train loss:2.086577354330648\n",
      "train loss:2.1233746123037656\n",
      "train loss:2.1460826672810067\n",
      "=== epoch:110, train acc:0.31, test acc:0.2486 ===\n",
      "train loss:2.1650901455064773\n",
      "train loss:2.1338983176845283\n",
      "train loss:2.1710175332925816\n",
      "=== epoch:111, train acc:0.31, test acc:0.2502 ===\n",
      "train loss:2.145680338115733\n",
      "train loss:2.140435233744267\n",
      "train loss:2.180906281981958\n",
      "=== epoch:112, train acc:0.32666666666666666, test acc:0.2544 ===\n",
      "train loss:2.1689933190434716\n",
      "train loss:2.156022172113126\n",
      "train loss:2.143287981801704\n",
      "=== epoch:113, train acc:0.33666666666666667, test acc:0.2596 ===\n",
      "train loss:2.1291748960098795\n",
      "train loss:2.1808853316793875\n",
      "train loss:2.165622011446842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:114, train acc:0.3433333333333333, test acc:0.2628 ===\n",
      "train loss:2.1441380677113164\n",
      "train loss:2.1344329482229343\n",
      "train loss:2.1657170858978843\n",
      "=== epoch:115, train acc:0.35333333333333333, test acc:0.271 ===\n",
      "train loss:2.1079476307493503\n",
      "train loss:2.164193962533832\n",
      "train loss:2.1393782690565475\n",
      "=== epoch:116, train acc:0.35333333333333333, test acc:0.2706 ===\n",
      "train loss:2.1565210356283875\n",
      "train loss:2.166634941345181\n",
      "train loss:2.154070301102776\n",
      "=== epoch:117, train acc:0.36333333333333334, test acc:0.2781 ===\n",
      "train loss:2.1421335431511666\n",
      "train loss:2.1589127850043717\n",
      "train loss:2.118697562156901\n",
      "=== epoch:118, train acc:0.36, test acc:0.2798 ===\n",
      "train loss:2.1222572349292417\n",
      "train loss:2.1721254336765417\n",
      "train loss:2.134958873734239\n",
      "=== epoch:119, train acc:0.36333333333333334, test acc:0.28 ===\n",
      "train loss:2.147658116990273\n",
      "train loss:2.117605308575743\n",
      "train loss:2.1377532917397937\n",
      "=== epoch:120, train acc:0.36, test acc:0.2815 ===\n",
      "train loss:2.144227649390909\n",
      "train loss:2.1457272729309924\n",
      "train loss:2.179413613411087\n",
      "=== epoch:121, train acc:0.36666666666666664, test acc:0.2892 ===\n",
      "train loss:2.11841520905169\n",
      "train loss:2.1299151348644774\n",
      "train loss:2.1260517673077666\n",
      "=== epoch:122, train acc:0.37333333333333335, test acc:0.2908 ===\n",
      "train loss:2.1104481067361878\n",
      "train loss:2.138759238901329\n",
      "train loss:2.1001839679504553\n",
      "=== epoch:123, train acc:0.37666666666666665, test acc:0.2927 ===\n",
      "train loss:2.1470942281111802\n",
      "train loss:2.101821581833272\n",
      "train loss:2.140717519914804\n",
      "=== epoch:124, train acc:0.3933333333333333, test acc:0.2974 ===\n",
      "train loss:2.0852084053639346\n",
      "train loss:2.1366470462359772\n",
      "train loss:2.115664833080979\n",
      "=== epoch:125, train acc:0.38, test acc:0.2955 ===\n",
      "train loss:2.103946190470746\n",
      "train loss:2.144442586091441\n",
      "train loss:2.0949870805627033\n",
      "=== epoch:126, train acc:0.38, test acc:0.2971 ===\n",
      "train loss:2.128095153922453\n",
      "train loss:2.112781057956102\n",
      "train loss:2.168569058647775\n",
      "=== epoch:127, train acc:0.38333333333333336, test acc:0.3011 ===\n",
      "train loss:2.110056229908242\n",
      "train loss:2.1601733059187462\n",
      "train loss:2.1582397408994343\n",
      "=== epoch:128, train acc:0.39666666666666667, test acc:0.3047 ===\n",
      "train loss:2.0885334956475194\n",
      "train loss:2.0993253685586426\n",
      "train loss:2.1233755683577447\n",
      "=== epoch:129, train acc:0.39, test acc:0.3023 ===\n",
      "train loss:2.1267357898326975\n",
      "train loss:2.091180392858521\n",
      "train loss:2.089314591388499\n",
      "=== epoch:130, train acc:0.39666666666666667, test acc:0.3037 ===\n",
      "train loss:2.1465326725642986\n",
      "train loss:2.139921416620159\n",
      "train loss:2.1103828087922083\n",
      "=== epoch:131, train acc:0.4, test acc:0.3085 ===\n",
      "train loss:2.0859142380658455\n",
      "train loss:2.154908517849502\n",
      "train loss:2.070370513065361\n",
      "=== epoch:132, train acc:0.39666666666666667, test acc:0.3073 ===\n",
      "train loss:2.0782486787903913\n",
      "train loss:2.08385637491744\n",
      "train loss:2.1070964357269717\n",
      "=== epoch:133, train acc:0.39666666666666667, test acc:0.3076 ===\n",
      "train loss:2.1060308341427536\n",
      "train loss:2.1113664920280133\n",
      "train loss:2.0918432413544235\n",
      "=== epoch:134, train acc:0.4066666666666667, test acc:0.3107 ===\n",
      "train loss:2.103745442829552\n",
      "train loss:2.1080794371468405\n",
      "train loss:2.066537271047972\n",
      "=== epoch:135, train acc:0.4166666666666667, test acc:0.3125 ===\n",
      "train loss:2.121017103836843\n",
      "train loss:2.0512338432118646\n",
      "train loss:2.1018082950434978\n",
      "=== epoch:136, train acc:0.41333333333333333, test acc:0.3167 ===\n",
      "train loss:2.079710618480399\n",
      "train loss:2.089306982475152\n",
      "train loss:2.094481319306295\n",
      "=== epoch:137, train acc:0.4166666666666667, test acc:0.3168 ===\n",
      "train loss:2.0722868774584624\n",
      "train loss:2.044573566307321\n",
      "train loss:2.0359000336921933\n",
      "=== epoch:138, train acc:0.4066666666666667, test acc:0.3157 ===\n",
      "train loss:2.0362208164650877\n",
      "train loss:2.0952801018726896\n",
      "train loss:2.1188459208179484\n",
      "=== epoch:139, train acc:0.4033333333333333, test acc:0.3163 ===\n",
      "train loss:2.088283035912789\n",
      "train loss:2.0604229738900366\n",
      "train loss:2.056113154781663\n",
      "=== epoch:140, train acc:0.41, test acc:0.3198 ===\n",
      "train loss:2.082082548971428\n",
      "train loss:2.019807597296369\n",
      "train loss:2.078955618400782\n",
      "=== epoch:141, train acc:0.4066666666666667, test acc:0.3218 ===\n",
      "train loss:2.1100175407883426\n",
      "train loss:2.116644962272851\n",
      "train loss:2.014386099578055\n",
      "=== epoch:142, train acc:0.4066666666666667, test acc:0.3239 ===\n",
      "train loss:2.0956051139935474\n",
      "train loss:2.0453382695687976\n",
      "train loss:1.9684052124614493\n",
      "=== epoch:143, train acc:0.4066666666666667, test acc:0.3243 ===\n",
      "train loss:2.05441680867608\n",
      "train loss:2.033718668087236\n",
      "train loss:2.074103080151806\n",
      "=== epoch:144, train acc:0.42, test acc:0.3313 ===\n",
      "train loss:2.0737403345561924\n",
      "train loss:2.0583295110047835\n",
      "train loss:2.036972711907944\n",
      "=== epoch:145, train acc:0.42333333333333334, test acc:0.3334 ===\n",
      "train loss:2.0728418394181802\n",
      "train loss:2.0526661263177464\n",
      "train loss:2.013524581955922\n",
      "=== epoch:146, train acc:0.4266666666666667, test acc:0.3351 ===\n",
      "train loss:1.9983590416398158\n",
      "train loss:2.080452511239712\n",
      "train loss:2.087188615855109\n",
      "=== epoch:147, train acc:0.4266666666666667, test acc:0.3367 ===\n",
      "train loss:2.0460254803914677\n",
      "train loss:2.0776008768075127\n",
      "train loss:2.02880064145226\n",
      "=== epoch:148, train acc:0.43, test acc:0.3413 ===\n",
      "train loss:2.0523070353035933\n",
      "train loss:2.078418953341978\n",
      "train loss:2.0652315627706437\n",
      "=== epoch:149, train acc:0.44333333333333336, test acc:0.3469 ===\n",
      "train loss:1.9765737856093701\n",
      "train loss:2.112990566781018\n",
      "train loss:2.053356172955942\n",
      "=== epoch:150, train acc:0.44333333333333336, test acc:0.3495 ===\n",
      "train loss:1.9654291166513347\n",
      "train loss:2.075769082928851\n",
      "train loss:2.0234741287756237\n",
      "=== epoch:151, train acc:0.43333333333333335, test acc:0.3497 ===\n",
      "train loss:1.9793436607752612\n",
      "train loss:2.013804380392627\n",
      "train loss:2.131379251149603\n",
      "=== epoch:152, train acc:0.44666666666666666, test acc:0.3574 ===\n",
      "train loss:2.043221241605596\n",
      "train loss:2.041540821330904\n",
      "train loss:2.0041959349378358\n",
      "=== epoch:153, train acc:0.44333333333333336, test acc:0.3523 ===\n",
      "train loss:1.9828991282888768\n",
      "train loss:1.986907804186329\n",
      "train loss:2.0232167869669224\n",
      "=== epoch:154, train acc:0.44666666666666666, test acc:0.3542 ===\n",
      "train loss:2.041066914672224\n",
      "train loss:2.0254691052286247\n",
      "train loss:1.9882373454868725\n",
      "=== epoch:155, train acc:0.4533333333333333, test acc:0.3604 ===\n",
      "train loss:2.0348050033017073\n",
      "train loss:2.038329390717687\n",
      "train loss:1.9297908265813504\n",
      "=== epoch:156, train acc:0.45, test acc:0.358 ===\n",
      "train loss:1.9783344166654444\n",
      "train loss:2.0012098915257037\n",
      "train loss:1.9593547649108496\n",
      "=== epoch:157, train acc:0.4533333333333333, test acc:0.3601 ===\n",
      "train loss:2.0503684937059674\n",
      "train loss:2.0074563688152667\n",
      "train loss:2.0181828271337836\n",
      "=== epoch:158, train acc:0.45, test acc:0.3577 ===\n",
      "train loss:1.9724226983064235\n",
      "train loss:1.9918826412937847\n",
      "train loss:2.0411532895321995\n",
      "=== epoch:159, train acc:0.4533333333333333, test acc:0.3637 ===\n",
      "train loss:1.956147888667764\n",
      "train loss:1.9854885899186256\n",
      "train loss:1.957890144646707\n",
      "=== epoch:160, train acc:0.46, test acc:0.3643 ===\n",
      "train loss:2.002298072139612\n",
      "train loss:2.0369857114509053\n",
      "train loss:1.9562661718757448\n",
      "=== epoch:161, train acc:0.4633333333333333, test acc:0.3682 ===\n",
      "train loss:1.9498815074498412\n",
      "train loss:1.960982329940357\n",
      "train loss:2.0478685745807903\n",
      "=== epoch:162, train acc:0.4666666666666667, test acc:0.373 ===\n",
      "train loss:2.0067911468013264\n",
      "train loss:1.9987522817223797\n",
      "train loss:2.0004890019418173\n",
      "=== epoch:163, train acc:0.47, test acc:0.3768 ===\n",
      "train loss:1.9330819125321\n",
      "train loss:1.9548877222894672\n",
      "train loss:1.9642219120185485\n",
      "=== epoch:164, train acc:0.47, test acc:0.3762 ===\n",
      "train loss:2.0377882182872913\n",
      "train loss:2.028617386735178\n",
      "train loss:2.0241252981270095\n",
      "=== epoch:165, train acc:0.4766666666666667, test acc:0.3813 ===\n",
      "train loss:1.9733493013724495\n",
      "train loss:1.9776728231441743\n",
      "train loss:1.9623552821440189\n",
      "=== epoch:166, train acc:0.47333333333333333, test acc:0.3831 ===\n",
      "train loss:1.9304305886388011\n",
      "train loss:1.9097417447976388\n",
      "train loss:2.0102147363324296\n",
      "=== epoch:167, train acc:0.4766666666666667, test acc:0.3864 ===\n",
      "train loss:1.959402410972765\n",
      "train loss:1.9939470391767171\n",
      "train loss:1.997040673803159\n",
      "=== epoch:168, train acc:0.49, test acc:0.3953 ===\n",
      "train loss:1.8725560427939443\n",
      "train loss:1.8918231177738727\n",
      "train loss:1.9139857256044812\n",
      "=== epoch:169, train acc:0.5, test acc:0.3936 ===\n",
      "train loss:1.9862140734680815\n",
      "train loss:1.873453143251513\n",
      "train loss:1.9246408420700731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:170, train acc:0.5, test acc:0.3953 ===\n",
      "train loss:1.8603048078455773\n",
      "train loss:2.007057957324978\n",
      "train loss:1.9279828878997964\n",
      "=== epoch:171, train acc:0.5033333333333333, test acc:0.4003 ===\n",
      "train loss:1.9115248954249873\n",
      "train loss:1.8258141095837142\n",
      "train loss:1.9423592811169201\n",
      "=== epoch:172, train acc:0.5, test acc:0.4027 ===\n",
      "train loss:1.905769121077673\n",
      "train loss:1.8479398132608753\n",
      "train loss:1.9169056985920856\n",
      "=== epoch:173, train acc:0.5033333333333333, test acc:0.4125 ===\n",
      "train loss:1.926953537820923\n",
      "train loss:1.9988848085127189\n",
      "train loss:1.861990718941384\n",
      "=== epoch:174, train acc:0.5033333333333333, test acc:0.4164 ===\n",
      "train loss:1.9510959316798142\n",
      "train loss:1.9444504810810976\n",
      "train loss:1.9138438037455712\n",
      "=== epoch:175, train acc:0.51, test acc:0.4197 ===\n",
      "train loss:1.9455383298963589\n",
      "train loss:1.9157252294851674\n",
      "train loss:1.989250086830112\n",
      "=== epoch:176, train acc:0.52, test acc:0.4267 ===\n",
      "train loss:1.9021952120462413\n",
      "train loss:1.8347699697785131\n",
      "train loss:1.8542175317686298\n",
      "=== epoch:177, train acc:0.53, test acc:0.4273 ===\n",
      "train loss:1.9101242676851302\n",
      "train loss:1.8954433504602957\n",
      "train loss:1.91831406038881\n",
      "=== epoch:178, train acc:0.5266666666666666, test acc:0.4284 ===\n",
      "train loss:1.9538852476820971\n",
      "train loss:1.88283427170172\n",
      "train loss:1.8939675598641146\n",
      "=== epoch:179, train acc:0.5366666666666666, test acc:0.4299 ===\n",
      "train loss:1.8511223015591238\n",
      "train loss:1.8636748233449854\n",
      "train loss:1.794951339628086\n",
      "=== epoch:180, train acc:0.5333333333333333, test acc:0.4298 ===\n",
      "train loss:1.8561641870874537\n",
      "train loss:1.9636229555146747\n",
      "train loss:1.8745226347913206\n",
      "=== epoch:181, train acc:0.5333333333333333, test acc:0.433 ===\n",
      "train loss:1.8474107475251045\n",
      "train loss:1.8133534435633234\n",
      "train loss:1.83677836857421\n",
      "=== epoch:182, train acc:0.5266666666666666, test acc:0.4353 ===\n",
      "train loss:1.9546099157519896\n",
      "train loss:1.8309852557932045\n",
      "train loss:1.9018452946957303\n",
      "=== epoch:183, train acc:0.5333333333333333, test acc:0.4406 ===\n",
      "train loss:1.8799934324087848\n",
      "train loss:1.9717955268737697\n",
      "train loss:1.863177442918684\n",
      "=== epoch:184, train acc:0.5366666666666666, test acc:0.4406 ===\n",
      "train loss:1.905846824275092\n",
      "train loss:1.8182173472118521\n",
      "train loss:1.8012876776249598\n",
      "=== epoch:185, train acc:0.54, test acc:0.4433 ===\n",
      "train loss:1.862198411035013\n",
      "train loss:1.818280974854597\n",
      "train loss:1.8655589819968297\n",
      "=== epoch:186, train acc:0.5366666666666666, test acc:0.4473 ===\n",
      "train loss:1.829029128362849\n",
      "train loss:1.8564858056662257\n",
      "train loss:1.7810375951967128\n",
      "=== epoch:187, train acc:0.5333333333333333, test acc:0.447 ===\n",
      "train loss:1.86416988426385\n",
      "train loss:1.8734051918088526\n",
      "train loss:1.790413334350506\n",
      "=== epoch:188, train acc:0.53, test acc:0.4423 ===\n",
      "train loss:1.7756919771182584\n",
      "train loss:1.7313875715160907\n",
      "train loss:1.9239861661041258\n",
      "=== epoch:189, train acc:0.5333333333333333, test acc:0.4463 ===\n",
      "train loss:1.8709165431359265\n",
      "train loss:1.8343471326597356\n",
      "train loss:1.7905014604331229\n",
      "=== epoch:190, train acc:0.5366666666666666, test acc:0.4474 ===\n",
      "train loss:1.7375693904975669\n",
      "train loss:1.8419753700758696\n",
      "train loss:1.8651588011932736\n",
      "=== epoch:191, train acc:0.5433333333333333, test acc:0.4535 ===\n",
      "train loss:1.7911868397064286\n",
      "train loss:1.8457396321259856\n",
      "train loss:1.875449195595516\n",
      "=== epoch:192, train acc:0.5466666666666666, test acc:0.4562 ===\n",
      "train loss:1.8053704051897244\n",
      "train loss:1.790107436905271\n",
      "train loss:1.726390792794636\n",
      "=== epoch:193, train acc:0.5566666666666666, test acc:0.4591 ===\n",
      "train loss:1.7820540265469527\n",
      "train loss:1.738729019317362\n",
      "train loss:1.864077839883703\n",
      "=== epoch:194, train acc:0.56, test acc:0.4597 ===\n",
      "train loss:1.7374924752099772\n",
      "train loss:1.8276359390577201\n",
      "train loss:1.8744538757657725\n",
      "=== epoch:195, train acc:0.5733333333333334, test acc:0.464 ===\n",
      "train loss:1.9509601932528648\n",
      "train loss:1.7446000658719785\n",
      "train loss:1.782964070831911\n",
      "=== epoch:196, train acc:0.5733333333333334, test acc:0.4668 ===\n",
      "train loss:1.6963543340485319\n",
      "train loss:1.7964811568099148\n",
      "train loss:1.779844532255185\n",
      "=== epoch:197, train acc:0.5633333333333334, test acc:0.4618 ===\n",
      "train loss:1.7199522463072403\n",
      "train loss:1.7377698549288576\n",
      "train loss:1.790221588649206\n",
      "=== epoch:198, train acc:0.5633333333333334, test acc:0.461 ===\n",
      "train loss:1.7589363223613241\n",
      "train loss:1.7811226491487846\n",
      "train loss:1.8004686135833468\n",
      "=== epoch:199, train acc:0.56, test acc:0.4615 ===\n",
      "train loss:1.7280795224566412\n",
      "train loss:1.6768878696144522\n",
      "train loss:1.7639254117725032\n",
      "=== epoch:200, train acc:0.5566666666666666, test acc:0.463 ===\n",
      "train loss:1.7818362865305202\n",
      "train loss:1.8815856282165109\n",
      "train loss:1.7798229110090262\n",
      "=== epoch:201, train acc:0.56, test acc:0.4649 ===\n",
      "train loss:1.7613461039943947\n",
      "train loss:1.7802153559539837\n",
      "train loss:1.8643027972212025\n",
      "=== epoch:202, train acc:0.57, test acc:0.4694 ===\n",
      "train loss:1.7937551232574063\n",
      "train loss:1.7041191729353724\n",
      "train loss:1.7792883537815\n",
      "=== epoch:203, train acc:0.5733333333333334, test acc:0.4704 ===\n",
      "train loss:1.8429233577041473\n",
      "train loss:1.7632293931472354\n",
      "train loss:1.793937930631928\n",
      "=== epoch:204, train acc:0.5733333333333334, test acc:0.474 ===\n",
      "train loss:1.7192535462962595\n",
      "train loss:1.6941612638610861\n",
      "train loss:1.7184107554389172\n",
      "=== epoch:205, train acc:0.5766666666666667, test acc:0.475 ===\n",
      "train loss:1.7064086502867943\n",
      "train loss:1.7313960947401585\n",
      "train loss:1.7668065177510655\n",
      "=== epoch:206, train acc:0.5833333333333334, test acc:0.4816 ===\n",
      "train loss:1.7311468648745023\n",
      "train loss:1.6933945732715892\n",
      "train loss:1.7763351712817828\n",
      "=== epoch:207, train acc:0.5766666666666667, test acc:0.4805 ===\n",
      "train loss:1.7658556388571973\n",
      "train loss:1.8475646787249713\n",
      "train loss:1.794593277200842\n",
      "=== epoch:208, train acc:0.5866666666666667, test acc:0.4837 ===\n",
      "train loss:1.7262088755205973\n",
      "train loss:1.7637700595007462\n",
      "train loss:1.808127657792642\n",
      "=== epoch:209, train acc:0.5833333333333334, test acc:0.4829 ===\n",
      "train loss:1.8118841152295673\n",
      "train loss:1.7129822023238208\n",
      "train loss:1.6781471050103045\n",
      "=== epoch:210, train acc:0.5833333333333334, test acc:0.4844 ===\n",
      "train loss:1.6871852756900119\n",
      "train loss:1.579761527663949\n",
      "train loss:1.7701072624198224\n",
      "=== epoch:211, train acc:0.59, test acc:0.4791 ===\n",
      "train loss:1.7097112944243569\n",
      "train loss:1.6483812558989897\n",
      "train loss:1.7936715867007076\n",
      "=== epoch:212, train acc:0.5933333333333334, test acc:0.4845 ===\n",
      "train loss:1.8258207359926835\n",
      "train loss:1.8044010056241846\n",
      "train loss:1.6414342229582148\n",
      "=== epoch:213, train acc:0.6033333333333334, test acc:0.485 ===\n",
      "train loss:1.6264991330464753\n",
      "train loss:1.7113041338247141\n",
      "train loss:1.7150570234906215\n",
      "=== epoch:214, train acc:0.6, test acc:0.4864 ===\n",
      "train loss:1.750747225062608\n",
      "train loss:1.7410253396296433\n",
      "train loss:1.6555420433627464\n",
      "=== epoch:215, train acc:0.6, test acc:0.4878 ===\n",
      "train loss:1.6699227161299965\n",
      "train loss:1.6734177081160468\n",
      "train loss:1.6471596209792607\n",
      "=== epoch:216, train acc:0.6066666666666667, test acc:0.4884 ===\n",
      "train loss:1.6520660638601483\n",
      "train loss:1.6337270827262278\n",
      "train loss:1.5337943757109866\n",
      "=== epoch:217, train acc:0.6066666666666667, test acc:0.492 ===\n",
      "train loss:1.6491803926478672\n",
      "train loss:1.6603457181767\n",
      "train loss:1.6314385031697294\n",
      "=== epoch:218, train acc:0.61, test acc:0.4944 ===\n",
      "train loss:1.6994048156330477\n",
      "train loss:1.7168915927396007\n",
      "train loss:1.630396339398322\n",
      "=== epoch:219, train acc:0.61, test acc:0.4963 ===\n",
      "train loss:1.7491193332276382\n",
      "train loss:1.6219109621120174\n",
      "train loss:1.6479951866048699\n",
      "=== epoch:220, train acc:0.61, test acc:0.4986 ===\n",
      "train loss:1.6046812583698518\n",
      "train loss:1.7419803495027526\n",
      "train loss:1.6868025224371355\n",
      "=== epoch:221, train acc:0.61, test acc:0.4995 ===\n",
      "train loss:1.6430154869874574\n",
      "train loss:1.7071641065763625\n",
      "train loss:1.5625487751975762\n",
      "=== epoch:222, train acc:0.61, test acc:0.5005 ===\n",
      "train loss:1.6716168219066505\n",
      "train loss:1.7555737851647608\n",
      "train loss:1.5388826301885863\n",
      "=== epoch:223, train acc:0.61, test acc:0.4998 ===\n",
      "train loss:1.5763696471359065\n",
      "train loss:1.639378749693475\n",
      "train loss:1.591993139116819\n",
      "=== epoch:224, train acc:0.6133333333333333, test acc:0.505 ===\n",
      "train loss:1.7046277378592487\n",
      "train loss:1.5081496521552062\n",
      "train loss:1.7614576880329857\n",
      "=== epoch:225, train acc:0.61, test acc:0.5015 ===\n",
      "train loss:1.637580939082285\n",
      "train loss:1.640411950660238\n",
      "train loss:1.514601963024144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:226, train acc:0.6166666666666667, test acc:0.5028 ===\n",
      "train loss:1.643464049192733\n",
      "train loss:1.58505320644979\n",
      "train loss:1.5518155450132962\n",
      "=== epoch:227, train acc:0.6133333333333333, test acc:0.5047 ===\n",
      "train loss:1.6156960700242788\n",
      "train loss:1.5057807964721812\n",
      "train loss:1.6755833925314667\n",
      "=== epoch:228, train acc:0.6133333333333333, test acc:0.5047 ===\n",
      "train loss:1.6417873491088386\n",
      "train loss:1.4917616191050178\n",
      "train loss:1.5968964306142928\n",
      "=== epoch:229, train acc:0.6133333333333333, test acc:0.5067 ===\n",
      "train loss:1.5551326075210985\n",
      "train loss:1.5537197155530935\n",
      "train loss:1.5306389995151455\n",
      "=== epoch:230, train acc:0.61, test acc:0.5071 ===\n",
      "train loss:1.5500441693770661\n",
      "train loss:1.6637242364661906\n",
      "train loss:1.6315776553465184\n",
      "=== epoch:231, train acc:0.61, test acc:0.5074 ===\n",
      "train loss:1.6404015172094168\n",
      "train loss:1.551962254888582\n",
      "train loss:1.6447161057579212\n",
      "=== epoch:232, train acc:0.6066666666666667, test acc:0.5078 ===\n",
      "train loss:1.6750708473647002\n",
      "train loss:1.5424946957978432\n",
      "train loss:1.6117695151713787\n",
      "=== epoch:233, train acc:0.61, test acc:0.5085 ===\n",
      "train loss:1.5287846466259243\n",
      "train loss:1.5860357928698448\n",
      "train loss:1.5299289931412974\n",
      "=== epoch:234, train acc:0.6133333333333333, test acc:0.5102 ===\n",
      "train loss:1.4699266370185131\n",
      "train loss:1.677976334305289\n",
      "train loss:1.5265502541855807\n",
      "=== epoch:235, train acc:0.6166666666666667, test acc:0.5092 ===\n",
      "train loss:1.478661272135514\n",
      "train loss:1.680509200422287\n",
      "train loss:1.5738711990287575\n",
      "=== epoch:236, train acc:0.6166666666666667, test acc:0.51 ===\n",
      "train loss:1.5483842150435552\n",
      "train loss:1.561938955021809\n",
      "train loss:1.4677105843401796\n",
      "=== epoch:237, train acc:0.6133333333333333, test acc:0.5131 ===\n",
      "train loss:1.4802227902705536\n",
      "train loss:1.5600746830762038\n",
      "train loss:1.4070710058584401\n",
      "=== epoch:238, train acc:0.62, test acc:0.5138 ===\n",
      "train loss:1.5237095751577252\n",
      "train loss:1.532878202252126\n",
      "train loss:1.500324715195655\n",
      "=== epoch:239, train acc:0.6133333333333333, test acc:0.5113 ===\n",
      "train loss:1.60092830585261\n",
      "train loss:1.5552125276528384\n",
      "train loss:1.4677883634530873\n",
      "=== epoch:240, train acc:0.6166666666666667, test acc:0.5127 ===\n",
      "train loss:1.5851456027829507\n",
      "train loss:1.5546297433465641\n",
      "train loss:1.6136621180849005\n",
      "=== epoch:241, train acc:0.6133333333333333, test acc:0.5129 ===\n",
      "train loss:1.6445762826392802\n",
      "train loss:1.471265245792639\n",
      "train loss:1.4727911313185118\n",
      "=== epoch:242, train acc:0.6166666666666667, test acc:0.5157 ===\n",
      "train loss:1.5015153573179578\n",
      "train loss:1.499433340994893\n",
      "train loss:1.435530011726301\n",
      "=== epoch:243, train acc:0.6233333333333333, test acc:0.5164 ===\n",
      "train loss:1.520188111622411\n",
      "train loss:1.5403585720876458\n",
      "train loss:1.568168197092387\n",
      "=== epoch:244, train acc:0.62, test acc:0.5172 ===\n",
      "train loss:1.4979249450744938\n",
      "train loss:1.4146384360153663\n",
      "train loss:1.496848902630541\n",
      "=== epoch:245, train acc:0.6233333333333333, test acc:0.5175 ===\n",
      "train loss:1.621343018374557\n",
      "train loss:1.450522823019498\n",
      "train loss:1.596424012752899\n",
      "=== epoch:246, train acc:0.62, test acc:0.5195 ===\n",
      "train loss:1.5464887145265738\n",
      "train loss:1.4183748595258958\n",
      "train loss:1.391692739876979\n",
      "=== epoch:247, train acc:0.6266666666666667, test acc:0.5209 ===\n",
      "train loss:1.577500756948639\n",
      "train loss:1.5705792334859638\n",
      "train loss:1.4764730416855054\n",
      "=== epoch:248, train acc:0.62, test acc:0.522 ===\n",
      "train loss:1.5316663158283188\n",
      "train loss:1.5719574686947941\n",
      "train loss:1.5464111211974594\n",
      "=== epoch:249, train acc:0.6266666666666667, test acc:0.5239 ===\n",
      "train loss:1.6028355727358323\n",
      "train loss:1.5233695030537353\n",
      "train loss:1.4267925831747952\n",
      "=== epoch:250, train acc:0.64, test acc:0.5224 ===\n",
      "train loss:1.504049095589658\n",
      "train loss:1.446256373230693\n",
      "train loss:1.310742547525585\n",
      "=== epoch:251, train acc:0.64, test acc:0.5246 ===\n",
      "train loss:1.4490136488530825\n",
      "train loss:1.3985808434972338\n",
      "train loss:1.4038097485184593\n",
      "=== epoch:252, train acc:0.6366666666666667, test acc:0.5259 ===\n",
      "train loss:1.4884713420976319\n",
      "train loss:1.5251889366199292\n",
      "train loss:1.4495869036489757\n",
      "=== epoch:253, train acc:0.6433333333333333, test acc:0.5263 ===\n",
      "train loss:1.4929045179732177\n",
      "train loss:1.3661181834311718\n",
      "train loss:1.410801516590883\n",
      "=== epoch:254, train acc:0.6433333333333333, test acc:0.527 ===\n",
      "train loss:1.3541494093233595\n",
      "train loss:1.5446819346864686\n",
      "train loss:1.4224302518495735\n",
      "=== epoch:255, train acc:0.64, test acc:0.5292 ===\n",
      "train loss:1.503278349345565\n",
      "train loss:1.3735113456364672\n",
      "train loss:1.5183773721112317\n",
      "=== epoch:256, train acc:0.64, test acc:0.5313 ===\n",
      "train loss:1.3372901868505607\n",
      "train loss:1.4315146704211714\n",
      "train loss:1.484820880766696\n",
      "=== epoch:257, train acc:0.6433333333333333, test acc:0.5323 ===\n",
      "train loss:1.4408500750537212\n",
      "train loss:1.3817205693938868\n",
      "train loss:1.4479997532638107\n",
      "=== epoch:258, train acc:0.64, test acc:0.5333 ===\n",
      "train loss:1.4072964164877322\n",
      "train loss:1.4802709743248692\n",
      "train loss:1.3244463268775393\n",
      "=== epoch:259, train acc:0.63, test acc:0.5346 ===\n",
      "train loss:1.4026727734878057\n",
      "train loss:1.426410602352153\n",
      "train loss:1.5819847364441932\n",
      "=== epoch:260, train acc:0.6366666666666667, test acc:0.5344 ===\n",
      "train loss:1.3465345435797922\n",
      "train loss:1.3516855565025343\n",
      "train loss:1.3598954430303687\n",
      "=== epoch:261, train acc:0.64, test acc:0.5354 ===\n",
      "train loss:1.3921202288772525\n",
      "train loss:1.5161483677186498\n",
      "train loss:1.389382230390877\n",
      "=== epoch:262, train acc:0.64, test acc:0.5349 ===\n",
      "train loss:1.4304287527081043\n",
      "train loss:1.4037133088254432\n",
      "train loss:1.5112309014906018\n",
      "=== epoch:263, train acc:0.6466666666666666, test acc:0.5344 ===\n",
      "train loss:1.4011286721430278\n",
      "train loss:1.3683522119019673\n",
      "train loss:1.3485607014064729\n",
      "=== epoch:264, train acc:0.6433333333333333, test acc:0.5336 ===\n",
      "train loss:1.4527964298502243\n",
      "train loss:1.3014505857850296\n",
      "train loss:1.4447495245247142\n",
      "=== epoch:265, train acc:0.64, test acc:0.5334 ===\n",
      "train loss:1.3401867458449666\n",
      "train loss:1.2379493321561668\n",
      "train loss:1.2560236369335605\n",
      "=== epoch:266, train acc:0.6366666666666667, test acc:0.5311 ===\n",
      "train loss:1.3638927492966202\n",
      "train loss:1.2972464589184916\n",
      "train loss:1.4408703996804448\n",
      "=== epoch:267, train acc:0.6466666666666666, test acc:0.5333 ===\n",
      "train loss:1.429584058380623\n",
      "train loss:1.3798982187311006\n",
      "train loss:1.3896983912862395\n",
      "=== epoch:268, train acc:0.6433333333333333, test acc:0.5349 ===\n",
      "train loss:1.3649575793458075\n",
      "train loss:1.303946870504256\n",
      "train loss:1.2778754692046146\n",
      "=== epoch:269, train acc:0.65, test acc:0.5369 ===\n",
      "train loss:1.3394072708701688\n",
      "train loss:1.3077071862755973\n",
      "train loss:1.263030272415774\n",
      "=== epoch:270, train acc:0.6433333333333333, test acc:0.5395 ===\n",
      "train loss:1.4312188385302433\n",
      "train loss:1.3680267061183518\n",
      "train loss:1.4093140117967795\n",
      "=== epoch:271, train acc:0.65, test acc:0.5383 ===\n",
      "train loss:1.2571664716768811\n",
      "train loss:1.4167332708078215\n",
      "train loss:1.3404572073951635\n",
      "=== epoch:272, train acc:0.6466666666666666, test acc:0.5396 ===\n",
      "train loss:1.4037819086127354\n",
      "train loss:1.272352028979366\n",
      "train loss:1.2775033895349417\n",
      "=== epoch:273, train acc:0.6433333333333333, test acc:0.5422 ===\n",
      "train loss:1.350602717841225\n",
      "train loss:1.2322548540715936\n",
      "train loss:1.2766398770286036\n",
      "=== epoch:274, train acc:0.6433333333333333, test acc:0.5406 ===\n",
      "train loss:1.4647899881745234\n",
      "train loss:1.4615946604777823\n",
      "train loss:1.3655897155922418\n",
      "=== epoch:275, train acc:0.6433333333333333, test acc:0.5423 ===\n",
      "train loss:1.3036708880325472\n",
      "train loss:1.2954197722086527\n",
      "train loss:1.172963635526583\n",
      "=== epoch:276, train acc:0.6466666666666666, test acc:0.5444 ===\n",
      "train loss:1.3503888350925437\n",
      "train loss:1.4052524626246563\n",
      "train loss:1.276159041344765\n",
      "=== epoch:277, train acc:0.6433333333333333, test acc:0.5447 ===\n",
      "train loss:1.4099284930070348\n",
      "train loss:1.2261276668246541\n",
      "train loss:1.2390642240592498\n",
      "=== epoch:278, train acc:0.6433333333333333, test acc:0.5461 ===\n",
      "train loss:1.3549958139177247\n",
      "train loss:1.233159247060773\n",
      "train loss:1.3131257863688748\n",
      "=== epoch:279, train acc:0.6433333333333333, test acc:0.5461 ===\n",
      "train loss:1.324857591714955\n",
      "train loss:1.130877632927523\n",
      "train loss:1.2647461217698384\n",
      "=== epoch:280, train acc:0.6433333333333333, test acc:0.5471 ===\n",
      "train loss:1.2308682600280703\n",
      "train loss:1.342691724616462\n",
      "train loss:1.2606043463533145\n",
      "=== epoch:281, train acc:0.6433333333333333, test acc:0.5471 ===\n",
      "train loss:1.2926795897670684\n",
      "train loss:1.261266141606947\n",
      "train loss:1.3077798345652685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:282, train acc:0.6433333333333333, test acc:0.5473 ===\n",
      "train loss:1.1690220235683482\n",
      "train loss:1.1497201943456643\n",
      "train loss:1.1777216984990309\n",
      "=== epoch:283, train acc:0.6433333333333333, test acc:0.5479 ===\n",
      "train loss:1.3172086022670055\n",
      "train loss:1.256990267484576\n",
      "train loss:1.1449252843199098\n",
      "=== epoch:284, train acc:0.6433333333333333, test acc:0.5473 ===\n",
      "train loss:1.3383812535090716\n",
      "train loss:1.1982722634193586\n",
      "train loss:1.2241994480430294\n",
      "=== epoch:285, train acc:0.6466666666666666, test acc:0.5499 ===\n",
      "train loss:1.2684484211155975\n",
      "train loss:1.2395406793754393\n",
      "train loss:1.2803246538881259\n",
      "=== epoch:286, train acc:0.6433333333333333, test acc:0.5485 ===\n",
      "train loss:1.150952583862347\n",
      "train loss:1.2854556986995505\n",
      "train loss:1.2199594223120218\n",
      "=== epoch:287, train acc:0.6433333333333333, test acc:0.5478 ===\n",
      "train loss:1.2337156676374088\n",
      "train loss:1.3088675791433713\n",
      "train loss:1.2721146498861795\n",
      "=== epoch:288, train acc:0.6433333333333333, test acc:0.5503 ===\n",
      "train loss:1.301714318410194\n",
      "train loss:1.3088206750157383\n",
      "train loss:1.3392997219685825\n",
      "=== epoch:289, train acc:0.6433333333333333, test acc:0.5507 ===\n",
      "train loss:1.2075453945344925\n",
      "train loss:1.2015410325690634\n",
      "train loss:1.1918947110015106\n",
      "=== epoch:290, train acc:0.6433333333333333, test acc:0.5502 ===\n",
      "train loss:1.368875327474763\n",
      "train loss:1.276406072110298\n",
      "train loss:1.2568197105554462\n",
      "=== epoch:291, train acc:0.64, test acc:0.5474 ===\n",
      "train loss:1.1890765605501208\n",
      "train loss:1.1137714630418012\n",
      "train loss:1.2975415693570367\n",
      "=== epoch:292, train acc:0.64, test acc:0.548 ===\n",
      "train loss:1.1855750644255043\n",
      "train loss:1.2861778754425606\n",
      "train loss:1.3531861602363096\n",
      "=== epoch:293, train acc:0.6466666666666666, test acc:0.5485 ===\n",
      "train loss:1.1383682088811702\n",
      "train loss:1.0423531192522295\n",
      "train loss:1.1376126961935047\n",
      "=== epoch:294, train acc:0.6433333333333333, test acc:0.5487 ===\n",
      "train loss:1.152659514621684\n",
      "train loss:1.265310931681465\n",
      "train loss:1.1178800839860867\n",
      "=== epoch:295, train acc:0.6466666666666666, test acc:0.5464 ===\n",
      "train loss:1.2304758030047693\n",
      "train loss:1.0949163879803345\n",
      "train loss:1.2923842098958445\n",
      "=== epoch:296, train acc:0.6466666666666666, test acc:0.5476 ===\n",
      "train loss:1.0791906877448234\n",
      "train loss:1.2216710066797254\n",
      "train loss:1.1711239242037905\n",
      "=== epoch:297, train acc:0.6466666666666666, test acc:0.5488 ===\n",
      "train loss:1.0729544630817813\n",
      "train loss:1.2133359315553054\n",
      "train loss:1.3086416951700242\n",
      "=== epoch:298, train acc:0.65, test acc:0.5508 ===\n",
      "train loss:1.1332508106138623\n",
      "train loss:1.1557279061634753\n",
      "train loss:1.1632311761113094\n",
      "=== epoch:299, train acc:0.6466666666666666, test acc:0.5519 ===\n",
      "train loss:1.1462807147354557\n",
      "train loss:1.246984663534124\n",
      "train loss:1.1567375699273548\n",
      "=== epoch:300, train acc:0.6466666666666666, test acc:0.5524 ===\n",
      "train loss:1.0612328152975974\n",
      "train loss:1.2264198196586147\n",
      "train loss:1.2189323978098106\n",
      "=== epoch:301, train acc:0.6566666666666666, test acc:0.5509 ===\n",
      "train loss:1.294472103616705\n",
      "train loss:1.1753161848641853\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5489\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 드롭아웃이란 매 학습시마다, layer에서 무작위로 몇개의 weight를 전달하지 않는 방식이다.\n",
    "# 가중치 감소법은 모델이 커지고 복잡성이 많아지면 혼자 감당하기 힘들다.\n",
    "\n",
    "# 드롭아웃 방식으로 weight이 쉽게 한쪽으로 쏠리며 커지는 것을 방지\n",
    "\n",
    "# 실제 시험을 할때에는 모든 weight을 전달하는데 단, 이때 출력에는 훈련때 삭제한 비율만큼을 곱하여 출력한다. (? 삭제하지 않은만큼의 비율을 곱해야, 총합이 훈련때와 같은것 아닌가!?)\n",
    "# 이도 학습속도를 늘게 하느 요소가 있기때문에 100센트에 도다하기 쉽지 안다.\n",
    "# 하지만 overfitting을 어제 할 수 있다 .!\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# 드롭아웃 사용 유무와 비울 설정 ========================\n",
    "use_dropout = True  # 드롭아웃을 쓰지 않을 때는 False\n",
    "dropout_ratio = 0.2\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 둘의 차이가 현저하게 줄어들었다.\n",
    "\n",
    "# 그래프 그리기==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
