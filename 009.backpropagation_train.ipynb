{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "from dataset.mnist import load_mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.functions import *\n",
    "from common.util import im2col, col2im\n",
    "\n",
    "#Relu 활성화함수\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "#Sigmoid 활성화함수\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx\n",
    "\n",
    "#Affine\n",
    "#foward 와 back의 다차원 행렬로의 확장.\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 가중치와 편향 매개변수의 미분\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 텐서 대응\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1) # shape[0]와 행을 맞추고, 열은 알아서 맞게 변경\n",
    "        self.x = x\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    #역전파\n",
    "    def backward(self, dout):\n",
    "        #dout이라는 전 미분값을 받으면..\n",
    "        dx = np.dot(dout, self.W.T)      #  입력값의 미분값\n",
    "                                         # 다음 깊이에서 dout이 될 수 있다.\n",
    "        \n",
    "        self.dW = np.dot(self.x.T, dout) # Weight의 미분값  !!중요,, weight의 행렬 shape가 같게 나온다,but task들의 개수만큼 미분값이 더해진다. \n",
    "                                         # x_T와 dout의 각 행과 열은  w = x*dS의 식에 맞는 짝인데, 행과 열 곱의 합이 dw에 들어간다. 즉 \n",
    "                                         # batch의 각 원소들의 dw해당값이 모두 더해져버렸다. 평균을 내야 좀더 학습이 세밀하게 된다!\n",
    "                                         # 그래서 처음 손실함수에서 받을때부터 batch만큼 값을 나누어서 준다!!\n",
    "        \n",
    "        self.db = np.sum(dout, axis=0)   # 편향에 대한 결과값 증가량은 전달받은 미분값만큼이다..\n",
    "                                         #  현 노드에서 bias의 영향력, 편미분값은 1이기 때문에\n",
    "                                         #  bias는 다음 노드들의 미분값만큼만 결과에 영향력을 행사할 수 있다 \n",
    "                                         #  즉 1* dout 이 bias가 가지는 영향력(결과에 대한 미분값)이다.\n",
    "                                         #  평균을 내기위해서 각 배치들의 미분 합 / 배치수를 하면맞다\n",
    "                                         #  그래서 batch 차원을 기준으로 합!!,,bias는 노드 수 만큼만 있다. 노드수 = output 수\n",
    "                                         #  처음 입력받을때 batch수만큼 나눴으므로 그냥 합해주면된다!!\n",
    "                                         \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
    "        return dx\n",
    "\n",
    "\n",
    "#학습을 위해 마지막 결과의 차이 필요\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실함수\n",
    "        self.y = None    # softmax의 출력\n",
    "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
    "        \n",
    "    # 순방향 출력시\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        #결과 값을 softmax로 출력하여 교차 엔트로피 오차 계산\n",
    "        self.y = softmax(x) \n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    #역방향시\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        \n",
    "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
    "            \n",
    "            # 오차 만큼을 전달해준다.\n",
    "            # softmax 함수의 손실함수가 cross-entropy일때 역전파는 y-t로 딱 떨어진다.\n",
    "            # 안에서 batch들의 미분값들이 더해질 것이므로 평균을 위해서는, batch_size로 처음에 나누어준 값을 준다.\n",
    "            \n",
    "             # !! 데이터는 batch만큼 한번에 들어가지만 weight는 한개만 존재!!\n",
    "            # 역전파 계산을 할때, w를 구하기위해 x인풋과 dout을 연산할 때 여러 데이터 미분값들의 합이 되어버린다 그래서 batch로 나눈다\n",
    "            # dW = x_T * dout 이 될텐데 이때, x와 dout은 batch data만큼의 정보를 가지고 있다.\n",
    "            # 행렬 곱시 x_T * dout의  x_T의 행과 dout의 열은 알맞는 x * dout 값 짝이며, W(행,열)에 그 미분값들의 합이 더해진다.\n",
    "            # 그래서 평균을 내기 위해 batch만큼 나눈다!!\n",
    "            \n",
    "            # 데이터 한개당 오차로 넘겨주는 것!\n",
    "            # 이 값을 굳이 batch_size로 나누지 않느다면\n",
    "            # 나중에 learning rate를 낮춰줘야한다... (기울기를 얼만큼 이동할지의 비율)\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)#*np.sqrt(2/hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)#*np.sqrt(2/output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict() #순서대로 Dict??\n",
    "        \n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        \n",
    "        #마지막층은 SoftMax\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    # 수치적 미분\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "\n",
    "    #오차역전파\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        #마지막 레이어에서는 1 미분값 전달.\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        \n",
    "        #dx를 dout으로 전달받으며 역방향으로 전달하며\n",
    "        # dW를 와 db를 구한다.\n",
    "        # 모든 gradient를 담은 dict를 반환\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 2.301491603781489\n",
      "0.0953 0.0897\n",
      "loss 0.40037327134014106\n",
      "0.9033666666666667 0.9068\n",
      "loss 0.2646801438180453\n",
      "0.9230666666666667 0.9277\n",
      "loss 0.27794835364350007\n",
      "0.9352333333333334 0.935\n",
      "loss 0.12316523043259706\n",
      "0.9448166666666666 0.9442\n",
      "loss 0.2355077385633875\n",
      "0.9507166666666667 0.9484\n",
      "loss 0.10412939955084566\n",
      "0.957 0.9545\n",
      "loss 0.15079937549189318\n",
      "0.9607333333333333 0.9562\n",
      "loss 0.1803403776231497\n",
      "0.9643833333333334 0.9588\n",
      "loss 0.14602527549987285\n",
      "0.9658666666666667 0.9614\n",
      "loss 0.06472236917544485\n",
      "0.9689833333333333 0.9622\n",
      "loss 0.05147109798387797\n",
      "0.9684166666666667 0.963\n",
      "loss 0.04621437133770941\n",
      "0.9733166666666667 0.9664\n",
      "loss 0.18197303263254447\n",
      "0.9748 0.9657\n",
      "loss 0.13113794922163138\n",
      "0.9766 0.9675\n",
      "loss 0.09852479506485931\n",
      "0.9761166666666666 0.9675\n",
      "loss 0.12968902906985463\n",
      "0.9780666666666666 0.9683\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
    "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n",
    "    \n",
    "    # 갱신\n",
    "    # 각 W 와 b의 현재 기울기* rate만큼 빼준다.\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        print(\"loss\",loss)\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
